[core][core][core]

# The folder where your airflow configuration lives

airflow_home = /opt/airflow# The folder where your airflow configuration lives# The folder where your airflow configuration lives



# The executor to use. Options include SequentialExecutor, LocalExecutor, CeleryExecutorairflow_home = /opt/airflowairflow_home = c:\Users\hynco\Desktop\SIGETI_DWH\airflow

executor = LocalExecutor



# Whether to load the DAG examples that ship with Airflow

load_examples = False# The executor to use. Options include SequentialExecutor, LocalExecutor, CeleryExecutor# The folder where airflow should store its log files



# The amount of parallelism as a setting to the executorexecutor = CeleryExecutorbase_log_folder = c:\Users\hynco\Desktop\SIGETI_DWH\airflow\logs

parallelism = 32



# The number of task instances allowed to run concurrently by the scheduler

dag_concurrency = 16# Whether to load the DAG examples that ship with Airflow# The folder where airflow looks for DAGs



# Are DAGs paused by default at creationload_examples = Falsedags_folder = c:\Users\hynco\Desktop\SIGETI_DWH\airflow\dags

dags_are_paused_at_creation = True



# The maximum number of active DAG runs per DAG

max_active_runs_per_dag = 16# The amount of parallelism as a setting to the executor# The folder where airflow looks for plugins



# The folder where airflow should store its log filesparallelism = 32plugins_folder = c:\Users\hynco\Desktop\SIGETI_DWH\airflow\plugins

base_log_folder = /opt/airflow/logs



# Airflow can store logs remotely in AWS S3, GCS or Elastic Search.

remote_logging = False# The number of task instances allowed to run concurrently by the scheduler# Airflow can store logs remotely in AWS S3, GCS or Elastic Search.



# Logging leveldag_concurrency = 16remote_logging = False

logging_level = INFO



# The SqlAlchemy connection string to the metadata database.

sql_alchemy_conn = postgresql+psycopg2://airflow:airflow123@postgres-airflow:5432/airflow# Are DAGs paused by default at creation# Logging level



[scheduler]dags_are_paused_at_creation = Truelogging_level = INFO

# Task instances listen for external kill signal (when you `airflow tasks kill`)

job_heartbeat_sec = 5



# The scheduler constantly tries to trigger new tasks# The maximum number of active DAG runs per DAG# Executor to use. Options include SequentialExecutor, LocalExecutor, CeleryExecutor

scheduler_heartbeat_sec = 5

max_active_runs_per_dag = 16executor = LocalExecutor

# How often (in seconds) to scan the DAGs directory for new files

dag_dir_list_interval = 300



[webserver]# The folder where airflow should store its log files# The SqlAlchemy connection string to the metadata database.

# The base URL of your website

base_url = http://localhost:8080base_log_folder = /opt/airflow/logs# For local development, SQLite is fine



# Number of seconds the webserver waits before killing gunicorn mastersql_alchemy_conn = sqlite:///c:\Users\hynco\Desktop\SIGETI_DWH\airflow\airflow.db

web_server_master_timeout = 120

# Airflow can store logs remotely in AWS S3, GCS or Elastic Search.

# Number of seconds the gunicorn webserver waits before timing out on a worker

web_server_worker_timeout = 120remote_logging = False# Whether to load DAGs in the web server



# The port on which to run the web serverload_examples = False

web_server_port = 8080

# Logging level

# Secret key used to run your flask app

secret_key = temporary_keylogging_level = INFO# Whether to expose the configuration file through the web server



# Whether to expose the configuration file through the web serverexpose_config = True

expose_config = True

# The SqlAlchemy connection string to the metadata database.

# Set to true to turn on authentication

authenticate = Falsesql_alchemy_conn = postgresql+psycopg2://airflow:airflow123@postgres-airflow:5432/airflow# Default timezone



# Filter the list of dags by owner namedefault_timezone = Europe/Paris

filter_by_owner = False
[celery]

# The Celery broker URL[scheduler]

broker_url = redis://redis:6379/0# Task instances listen for external kill signal (when you `airflow tasks kill`)

job_heartbeat_sec = 5

# The Celery result backend

result_backend = db+postgresql://airflow:airflow123@postgres-airflow:5432/airflow# The scheduler constantly tries to trigger new tasks (look at the scheduler_heartbeat_sec config)

scheduler_heartbeat_sec = 5

# Celery Flower is a sweet web-based tool for monitoring and administrating Celery clusters

flower_host = 0.0.0.0# How often (in seconds) to scan the DAGs directory for new files

flower_port = 5555dag_dir_list_interval = 300



# Default queue that tasks get assigned to and that worker listen on.# How often should stats be printed to the logs

default_queue = defaultprint_stats_interval = 30



# How many processes CeleryExecutor uses to sync task state.[webserver]

sync_parallelism = 0# The base URL of your website as airflow cannot guess what domain or

# cname you want to use. This is used in automated emails that

[scheduler]# airflow sends to point links to the right web server

# Task instances listen for external kill signal (when you `airflow tasks kill`)base_url = http://localhost:8080

job_heartbeat_sec = 5

# Default DAG view. Valid values are: tree, graph, duration, gantt, landing_times

# The scheduler constantly tries to trigger new tasksdefault_dag_view = graph

scheduler_heartbeat_sec = 5

# The port on which to run the web server

# How often (in seconds) to scan the DAGs directory for new filesweb_server_port = 8080

dag_dir_list_interval = 300

# Secret key used to run your flask app. It should be as random as possible

[webserver]secret_key = your_secret_key_here

# The base URL of your website

base_url = http://localhost:8080# Expose the configuration file in the web server

expose_config = True

# Number of seconds the webserver waits before killing gunicorn master

web_server_master_timeout = 120# Set to True to turn on authentication

authenticate = False

# Number of seconds the gunicorn webserver waits before timing out on a worker

web_server_worker_timeout = 120[email]

# Email backend to use

# The port on which to run the web serveremail_backend = airflow.utils.email.send_email_smtp

web_server_port = 8080

[smtp]

# Paths to the SSL certificate and key for the web server# If you want airflow to send emails on retries, failure, and you want to use

web_server_ssl_cert = # the airflow.utils.email.send_email_smtp function, you have to configure an

web_server_ssl_key = # smtp server here

smtp_host = localhost

# Whether to use SSL to connect to the web serversmtp_starttls = True

web_server_ssl_cert = smtp_ssl = False

smtp_port = 587

# Secret key used to run your flask appsmtp_mail_from = airflow@localhost
secret_key = temporary_key

# Whether to expose the configuration file through the web server
expose_config = True

# Set to true to turn on authentication
authenticate = False

# Filter the list of dags by owner name
filter_by_owner = False